{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27c6122",
   "metadata": {},
   "source": [
    "# Natural Language Processing 101 Workshop\n",
    "### Demo 2- Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3054d",
   "metadata": {},
   "source": [
    "In this demo, we'll be performing exploratory analysis on the expressed emotional \n",
    "sentiment of r/nyc users as embedded within their comment text. We will use readr, \n",
    "dplylr, and stringr once again as these packages are core libraries\n",
    "across NLP use cases. We're also including the **vader** package which wraps the original \n",
    "VADER sentiment analyser written in Python for R as well as **ggplot** for simple\n",
    "data visualizations. Let's load them all in along with our r/nyc comment data set as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(vader)\n",
    "library(ggplot2)\n",
    "\n",
    "nyc <- read_csv(\"nyc_reddit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fdff6b",
   "metadata": {},
   "source": [
    "Let’s consider three individual string examples in sequence to hone our intuition regarding how VADER generates its sentiment scores. We'll be using VADER's core function for analyzing the sentiment of individual records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5543b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vader(\"As someone who always depended on cars before, I LOVE the subway! <3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vader(\"The subway is very helpful, but I'm not a fan of the rats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de300bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vader(\"I hate how delayed the subway always is… being late for work sucks. :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc7038",
   "metadata": {},
   "source": [
    "We can see how each get_vader call generates both the individual scores of each word accounting for its polarity and valence, as well as the four overarching sentiment scores of each string. The first string is scored strongly positive and is shown to account for both the capitalization of “love” and the heart emoticon. The second string has both a positive connotation through “helpful” but also a negative tone with “not a fan,” leading to a weakly negative compound score. The final string is accurately identified as strongly negative and successfully captures the intention behind the sad face emoticon.\n",
    "\n",
    "Given our validation of VADER’s classification scheme to text very similar to our r/nyc comments, let’s go ahead and create a data frame of the VADER metrics of each of our comments. VADER has a designated function to generate its four sentiment scores across a full column of text that can then be used to instantiate an entirely seperate data frame dedicated to the results. This actual process however can take quite some time, so I've went ahead and pre-generated the results to be loaded in directly from another CSV for easy access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't actually run this- it takes a while to run \n",
    "# nyc_sentiment <- vader_df(nyc$body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_sentiment <- read_csv('r_nyc_10k_sentiment.csv')\n",
    "head(nyc_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153dd77",
   "metadata": {},
   "source": [
    "We can see through viewing the top of the generated data frame that the positive, negative, neutral, and compound scores have been identified for each respective comments. There's also a \"but_count\" comment to flag for sentiment negation- i.e. \"it's okay, but not my favorite\". \n",
    "\n",
    "Now that we have generated scores for the entirety of our dataset, let’s investigate what the most high-compound scoring positive and negative posts are respectively. We’ll start on the positive side first through some dplyr-powered data frame manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos <- nyc_sentiment %>%\n",
    "    top_n(5, compound)\n",
    "\n",
    "top_pos$text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ccd5",
   "metadata": {},
   "source": [
    "We can see through briefly reviewing the text that the most positive comments under VADER’s classification are speaking highly of particular locations or community initiatives or are expressing friendly comments to other users within the subreddit. The score is highly driven by the use of words such as \"better\", \"grand\", and \"well\" that the VADER dictionary has classified as positively valenced. \n",
    "\n",
    "Let’s replicate this for the most negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b32c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neg <- nyc_sentiment %>%\n",
    "    top_n(-5, compound)\n",
    "\n",
    "top_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95e798",
   "metadata": {},
   "source": [
    "We’ll skip reading through the details of the most negative comments due to their often disturbing content, but skimming the text lines gives a rather clear picture towards their general themes around crime within the city. The fact that these are the most negatively associated comments reflects VADER’s ability to catch the negative emotional valence of text related to violence and fear quite effectively.\n",
    "\n",
    "As a final exercise, we’ll explore whether there’s a relationship between an r/NYC post’s community score and its expressed sentiment as identified by VADER. Comments can either be upvoted or downvoted by other users. This produces a score that serves as a proxy for the collective community reaction to a given comment. \n",
    "\n",
    "To prepare for this analysis, I’ll first have to combine my separate data frames of the baseline Reddit data with the VADER scores by comment. Luckily, the 'body' column of comments can serve as a natural primary key for a simple join function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_full <- merge(nyc, nyc_sentiment, by.x = \"body\", by.y = \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca0d1a",
   "metadata": {},
   "source": [
    "We'll can now explore the sentiment trends of comments with a positive rating score of 20 through a ggplot scatter plot. You'll notice how the following code slices our dataset to our subpopulation of high-upvote comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(nyc_full[which(nyc_full$score>20),], aes(x=compound, y=score)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f3548",
   "metadata": {},
   "source": [
    "Let's look at the equivalent for sentiment among posts that received a negative score. We'll use the same ggplot call script by adjusting our conditional slice to capture the negative-scoring poster subgroup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(nyc_full[which(nyc_full$score<0),], aes(x=compound, y=score)) + geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe600e",
   "metadata": {},
   "source": [
    "And that's the fundamentals of exploring text data with a dictionary-based sentiment analyzer! As you can imagine there's a wide range of further extensions you can build from these foundations- looking at sentiment over time, across subgroups, before and after significant events, and beyond. Let's now return to our discussion of additional NLP methods back in the main workshop slides. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
