{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15aaaa7",
   "metadata": {},
   "source": [
    "# Natural Language Processing 101 Workshop\n",
    "### Demo 1- Text Pre-Processing\n",
    "\n",
    "In this demo, we'll be walking through how to perform the fundamentals of text \n",
    "pre-processing within R. Let's briefly go over the core packages that I've pre-downloaded \n",
    "into our session:\n",
    "* **readr** facilitates the easy uploading of external data files such as CSVs\n",
    "* **dplyr** promotes streamlined data frame creation and manipulation\n",
    "* **stringr** is all about working with text character strings\n",
    "* **tidytext** provides more specialized functions to prepare text data for various NLP use cases\n",
    "* **SnowballC** offers access to the popular Porter stemmer for word stemming\n",
    "\n",
    "These libraries are all great resources for performing essential text pre-processing\n",
    "functions. I'll also load in our working data set of 10,000 comments scraped from\n",
    "r/nyc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb034df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(readr): there is no package called ‘readr’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(readr): there is no package called ‘readr’\nTraceback:\n",
      "1. library(readr)"
     ]
    }
   ],
   "source": [
    "library(readr)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(tidytext)\n",
    "library(SnowballC)\n",
    "\n",
    "nyc <- read_csv(\"r_nyc_10k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527be4a",
   "metadata": {},
   "source": [
    "Let's check the character encoding of our r/nyc text data. The \"body\" column of the\n",
    "loaded data frame features the text of the scraped r/nyc comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_encoding(nyc$body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d6e0f",
   "metadata": {},
   "source": [
    "Our 1 confidence score on the UTF-8 encoding for the text column of our data frame \n",
    "assures us that the following pre-processing methods should work without \n",
    "problems around character encoding. Let’s now inspect our data set in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f64580",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(nyc) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cca068",
   "metadata": {},
   "source": [
    "This provides us with the first 10 records of our data set. “id” is the unique \n",
    "identifier for each comment, “author” provides the Reddit username who wrote the \n",
    "comment, “body” features the comments themselves, “score” refers to the \n",
    "Reddit system of being able to “upvote” or “downvote” comments similar to a \n",
    "like/dislike framework, and “date” as the year, month, and day the comment was \n",
    "posted. \n",
    "\n",
    "You'll notice with a preliminary view on the comments that there's unwanted noise\n",
    "within the text, such as '`&amp`' referring to the \"&\" symbol. Let's\n",
    "use a stringr function to remove all instances of '`&amp`' from the comment text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c371ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc$body <- str_remove_all(nyc$body, \"&amp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910571c9",
   "metadata": {},
   "source": [
    "This successfully removes all instances of “`&amp`” within the comments. There's a \n",
    "wide range of additional types of noise a researcher may want to remove from text. \n",
    "While our last str_remove_all command is quite effective at removing exact matches\n",
    "with '`&amp`', regexes are a particularly helpful alternative for matching a wider range\n",
    "of character types to delete within one function call. \n",
    "\n",
    "One common use case for regexes within text pre-processing is to remove all types \n",
    "of punctuation, since many NLP methods would otherwise consider tokenized punctuation\n",
    "to be their own unique words. Luckily, there are unique regex character classes that specialize\n",
    "in matching with groups of characters. Let's use one to identify and remove all \n",
    "punctuation from our text as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc$body <- str_remove_all(nyc$body, '[:punct:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a404c",
   "metadata": {},
   "source": [
    "It’s common within text pre-processing to convert all words to lowercase since \n",
    "capitalization may lead an NLP method to consider the same words as separate \n",
    "entities based on casing alone. You may also want to combine n-grams into \n",
    "single word units, such as concatenating all instances of the bi-gram “New York” \n",
    "into the single token of “New_York.” Let's learn two more stringr functions to \n",
    "accomplish both of these goals within our comment data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff20952",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc$body <- str_to_lower(nyc$body)\n",
    "\n",
    "nyc$body <- str_replace_all(nyc$body, \"New York\", \"New_York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3412ea3",
   "metadata": {},
   "source": [
    "*Tokenization* refers to the segmentation of text components into units such as \n",
    "individual words, sentences, paragraphs, characters, or n-grams. Let's use\n",
    "the tidytext package to create a new data frame of our r/nyc comments split\n",
    "into word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens <- nyc %>%\n",
    "    unnest_tokens(\"word\", body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c612783",
   "metadata": {},
   "source": [
    "Our 10k original comments are now seperated into unique rows of individual word \n",
    "tokens. By default, unnest_tokens lowercases the tokens and removes punctuation \n",
    "from the original string, which would allow you to skip our previous regex string\n",
    "removal workflow.\n",
    "\n",
    "Let's now consider the optional pre-processing steps we covered in the slides. \n",
    "First is *stopword removal*, which refers to common words such as \"and\", \"the\", \n",
    "and \"but\" that we'd like to remove from our text data since we consider\n",
    "them semantically irrelevant to our core NLP interests. Tidytext comes with a \n",
    "pre-made common stopwords list that we can call and apply directly to our generated\n",
    "tokens data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb540ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(stop_words)\n",
    "    \n",
    "tokens <- tokens %>%\n",
    "  anti_join(stop_words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e3a21",
   "metadata": {},
   "source": [
    "*Stemming* is used to manipulate text into its root word form, such as \"swimmers\"\n",
    "and \"swimming\" being stemmed to \"swim\". There’s a variety of different stemming \n",
    "algorithms available with the most commonly used being the Porter stemmer. We can\n",
    "use the methods from the SnowballC package to stem our own tokenized r/nyc comments \n",
    "as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems <- tokens %>%\n",
    "  mutate(stem = wordStem(word)) %>%\n",
    "  count(stem, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1416a3a",
   "metadata": {},
   "source": [
    "Finally, parts-of-speech (POS) tags identifies the grammatic and lexical classes \n",
    "that words in your text data belong to. There's a variety of different libraries \n",
    "that follow different POS tag rules, with the \"udpipe\"- referring to the Universal\n",
    "Dependencies tagging schema- being a popular library choice for R. \n",
    "\n",
    "We won't run this code chunk within the demo itself since it usually takes some\n",
    "time to process, but feel free to experiment with POS tags on your own time \n",
    "following the below work flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"udpipe\")\n",
    "\n",
    "## Donwload & load the POS tags for English\n",
    "model <- udpipe_download_model(language = \"english\")\n",
    "model <- udpipe_load_model(model)\n",
    "\n",
    "## Annotation will take quite some time, particularly if you're using the \n",
    "## non-stopword version of our tokenized dataset \n",
    "tags <- udpipe_annotate(model, x = tokens$word)\n",
    "\n",
    "## The generated \"upos\" column in the following dataframe will have each token's \n",
    "## identified POS tags.\n",
    "pos <- as.data.frame(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c3614",
   "metadata": {},
   "source": [
    "You've now completed the pre-processing demo and established a working knowledge\n",
    "of how to perform the fundamentals of text data pre-processing in R. Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
